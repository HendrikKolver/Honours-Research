\documentclass[11pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[T1]{fontenc}
\usepackage[margin=1.0in]{geometry}


\title{High quality content reconstruction in images using a high capacity self-embedding algorithm}

\author{Hendrik J Kolver}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}

\noindent Abstract of this paper and what I wrote in it.
This will give the reader a short overview of what they can expect to find in this paper.

\end{abstract}

\tableofcontents

\section{Introduction}
This section gives a brief description of the concepts used in this report.
The reasoning behind the proposed method will also be explained in \ref{introQualityIncrease} and \ref{introHighCap}.
This will serve to to show the applicability of the proposed method as well as the shortcomings it attempts to address. 

\subsection{Overview}
Self recovering images (described in \ref{introSelfEmbedRecovery}) poses an interesting problem.
There is an inherent trade-off (described in \ref{introQualityTrade}) between the quality of the image after embedding and the quality of the image after recovery.
This report proposes a method in which a high capacity embedding algorithm (described in \ref{introHighCap}) is applied to the problem of self-recovering images to attempt to increase the quality of the images (described in \ref{introQualityIncrease}) both after embedding and after recovery.


\subsection{Steganography}
\label{introSteganography}
Steganography is the process of hiding information in such a way that it does not get detected \cite{johnson1998exploring}.
Steganography, derived from Greek means "covered writing".
This means it differs from cryptography which does not attempt to hide information but merely scrambles it so it cannot be understood.
Steganography does not scramble the information but rather relies on the information to remain hidden. 

\subsection{Fragile Watermarking}
\label{introFragWatermarking}
Fragile watermarking is a variation of traditional watermarking where a watermark is embedded into an image such that subsequent alterations to the watermarked image can be detected with high probability \cite{lin1999review}.
This means that if modifications are made to the watermarked image it would be possible to detect that modifications were made as well as what parts of the image were modified.
This is a necessary component in self recovering images because in order to be able to recover image data it first needs to be determined what data needs to be recovered.
It thus needs to be know what data was modified.
A fragile watermark is used for this purpose. 

\subsection{Self embedding and recovery}
\label{introSelfEmbedRecovery}
Self embedding is a scheme where image content is embedded into the image itself. This embedded content can then later be used to recover damaged or modified parts of the image without accessing the original image itself \cite{fridrich1999images}.
A self embedding and recovery scheme such as the proposed method contains a couple of high level steps.
Firstly the authentic image content is embedded into the original image along with a fragile watermark.
This watermark will help determine later on which parts of the image was modified if any (see \ref{introFragWatermarking}).
When the authentic image needs to be recovered, after modification, the fragile watermark is used to determine which content was modified.
An attempt is then made to extract the original content for the areas that were modified from the modified image.
If this process is successful the user is left with a high quality reconstruction of the original image.

\subsection{Quality Trade-off}
\label{introQualityTrade}
There is an inherent quality trade-off present within self embedding schemes.
The trade-off exists between the image after embedding and the image after recovery.
If an embedding algorith such as LSB(Least significan bit) embedding is used (as in \ref{ErasureChannelOverview}) the actual bit values of the original image are overwritten.
Naturally the more bit values are overwritten the more noise is introduced into the image.
This effectively reduces the quality of the image after embedding.

\hspace{0pt} \\
The solution to this would be to simply embed a small amount of information into the image to keep the noise low.
However this presents another problem.
The quality of the image after recovery is dependent on the amount of authentic image data embedded into the image.

\hspace{0pt} \\
To achieve maximum quality after embedding the embedded information must be as little as possible.
To achieve maximum quality after recover the embedded information must be as large as possible to be as close as possible to the original image.
There exists then a trade-off between the two.

\subsubsection{Increasing quality with capacity}
\label{introQualityIncrease}
It might be possible to increase the quality of the image after recovery while still keeping the quality after embedding high.
It is proposed that applying an embedding algorithm designed for high capacity while retaining quality (as discussed in \ref{introHighCap}) to the problem of self embedding could possibly increase the quality of the image after embedding as well as after recovery.
This is because such an algorithm should provide higher embedding capacity while producing less noise in the image. 
 

\subsubsection{High capacity embedding algorithms}
\label{introHighCap}
The proposed method in this paper attempts to apply the algorithm for BPCS Steganography \cite{beaullieubpcs} to the problem of self recovering images, specifically to increase overall quality.

\hspace{0pt} \\
The goal of the BPCS algorithm is to embed as much data as possible into a cover image without detection \cite{beaullieubpcs}.
This refers to detection by human perception as well as statistical analysis although the latter is not the focus of this paper.
The BPCS algorithm uses an adaptive scheme to classify blocks that are suitable for embedding (similar to \ref{DynamicBlockOverview}).
BPCS steg differs however in the fact that the blocks can be conjugated in a checker board pattern to increase complexity and decrease detectability if needs be.

\hspace{0pt} \\
The BPCS algorithm can achieve a very high capacity while still remaining undetectable through human perception.
This makes the BPCS algorithm a possible good choice to increase the capacity of a self recovering image scheme and possible increase the quality through this process.


\section{Current self embedding and recovery schemes and algorithms}

This section compares current Content reconstruction algorithms using self embedding.
This serves to give an overview of what is currently available and to possibly highlight shortcomings of the current algorithms.
This would be useful to determine if the proposed method does indeed improve on existing methods. 
It would also help highlight strengths and shortcomings in the proposed method.
The methods analyzed were chosen because they all provide good quality cover and reconstructed images.
They also each offer an unique approach or aspect to the image recovery problem and would thus provide a good idea of the different approaches that have already been tested.

\subsection{Overview}

\subsubsection{Erasure channel model utilizing the remaining authentic content}
\label{ErasureChannelOverview}
The method proposed in \cite {korus2013efficient} uses an erasure channel as a model for the content reconstruction problem.
The method uses LSB embedding to embed the reference data into the image itself.
The method also uses a global spreading technique to spread the reference data across the image.
They also propose that by using the remaining authentic content in the image it is possible to have a high tamper rate while at the same time achieving good quality images before and after recovery.

\hspace{0pt} \\
The method proposed in \cite {korus2013efficient} achieves good quality cover images with a PSNR \textgreater 35dB. 
The method also achieves an image recovery quality of 35dB \textless PSNR \textgreater 40dB and 40dB \textless PSNR with taper rates of 50\% and 33\% respectively.
The method does not let the restoration quality of the image deteriorate much if the tampering rate is increased up to a value of 50\%.
They do however note that they can only achieve minimal reconstruction performance increases by decreasing the amount of information in the reconstruction reference.
By using only 50\% of the available capacity the maximal tampering rate increases form 50\% to 59\%.

\hspace{0pt} \\
This method \cite {korus2013efficient} is thus quite robust since 50\% of the image may be tampered with before recovery starts to deteriorate.
The security for this method is also very good since the quality of the cover image is not very susceptible to visual checks.
The authors did not do any statistical analysis on the image.
The embedding capacity of this method is also acceptable, but because the method uses some of the authentic image data to aid in the recovery the quality of the image, before and after recovery, is still very good even without a very high embedding capacity.  

\subsubsection{Method using difference expansion}
\label{differenceExpansionOverview}
\cite {tian2003high} Proposed a method that uses difference expansion and generalized LSB embedding.
Difference expansion works by exploiting the high redundancy that are present in images.
With difference expansion the payload is embedded in the difference of pixel values. 
For a pair of pixel values (x,y). \cite {tian2002reversible}
It should however be noted that image quality reduces rapidly as the payload size is increased when using difference expansion. 
The method uses these two techniques in combination to achieve a high embedding capacity while keeping distortion relatively low.
The method achieves a PSNR \textgreater 35dB after embedding.
The method achieves an embedding capacity of 1.78bpp when using up to the 4th LSB on a 512x512 8bit gray-scale version of the Lena image.

\hspace{0pt} \\
The method's \cite {tian2003high} restoration quality is acceptable at roughly 50\%. 
The difference expansion this method uses provides extra space for embedding.
The authors did thus not implement compression on the image data because of the extra space the difference expansion provides.
The embedding capacity of this method could thus be further improved by compressing the embedded data. This could possibly lead to better quality than what their experimental results achieved at the expense of complexity.
Difference expansion thus seems a good solution to increase the embedding capacity while still keeping the distortion low.

\hspace{0pt} \\ 
The authors do not mention the image tamper rate that this method \cite {tian2003high} allows.

\subsubsection{Dynamic block allocation for self embedding}
\label{DynamicBlockOverview}
(Qian et al) \cite{qian2011image} proposes a method for fragile watermarking with good restoration capabilities using self embedding.
The proposed method differs from the other methods mention in \ref{ErasureChannelOverview} and \ref{differenceExpansionOverview} due to the fact that the proposed method does not embed the information into all blocks uniformly.
The proposed method classifies different blocks of the image according to the block smoothness.
The less smooth the block is the more information will be embedded into it.
The authors argue that the current methods that use a fixed embedding size for each block are inadequate since less information should be embedded into the very smooth blocks and more should be embedded into the rough blocks \cite{qian2011image}.

\hspace{0pt} \\
The advantage of using this dynamic scheme is that there would be less visual distortion to the cover image. 
This is because if the specific block is already very rough (very busy visually) the human eye would not notice small changes.
If however the specific block is very smooth (very uniform visually) the human eye is more likely to notice small changes.
The proposed method thus creates less distortion in the cover image than fixed size methods while still retaining good restoration quality \cite{qian2011image}. 

\hspace{0pt} \\
The method proposed by (Qian et al) \cite{qian2011image} uses the 3 LSB bit planes to embed the needed information in.
The method achieves good results in experimentation with a cover image PSNR \textgreater 37dB as well as a reconstructed image PSNR = 35dB.
The method also allows for a tamper rate \textless 35\%.
The authors do not analyze the security of the algorithm.

\subsection{Comparison}
Each of the methods described in \ref{DynamicBlockOverview}, \ref{ErasureChannelOverview} and \ref{differenceExpansionOverview} provide good quality cover images as well as good quality reconstructed images.
There are however important differences.
The method described in \ref{ErasureChannelOverview} offers a very good tamper rate of 50\% compared to the method described in \ref{DynamicBlockOverview} which has a tamper rate of 35\%.
This means that a larger of the cove image can be tampered with while still being able to restore the image with good quality.

\hspace{0pt} \\
The method described in \ref{differenceExpansionOverview} achieves a cover image with a PSNR \textgreater 35dB, the method described in \ref{ErasureChannelOverview} achieves similar results, however the method described in \ref{DynamicBlockOverview} achieves a cover image PSNR \textgreater 37dB.
This means that the method described in \ref{DynamicBlockOverview} would have less noise in the cover images after embedding and would thus have better overall quality of the cover images.

\hspace{0pt} \\
At almost equal tamper rates of around 33\%-35\% the method described in \ref{ErasureChannelOverview} achieves a restoration PSNR \textgreater 40dB whereas the method described in \ref{DynamicBlockOverview} only achieves a restoration PSNR = 35dB.
The method described in \ref{differenceExpansionOverview} only achieves a restoration quality of about 50\% the original image.
This means that the method described in \ref{ErasureChannelOverview} would generally produce better quality restored images than the methods described in \ref{DynamicBlockOverview} and \ref{differenceExpansionOverview}.

\hspace{0pt} \\
This means that the method described in \ref{ErasureChannelOverview} provides the best tamper rate and the best restored image quality, and the method described in \ref{DynamicBlockOverview} provides the best cover image quality.
The method described in \ref{differenceExpansionOverview} provides decent quality of the cover image as well as the restored image.

\hspace{0pt} \\
All three methods thus provide good results and would serve as a good benchmark for comparing the method described later in this paper.

  

\bibliography{main}
\bibliographystyle{plain}

\end{document}
